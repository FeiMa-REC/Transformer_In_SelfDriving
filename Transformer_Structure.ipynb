{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer结构\n",
    "\n",
    "在上一节中系统介绍了Transformer中核心结构self-attention机制，得益于其计算可以轻松的转化为矩阵运算因此凭借GPU可以进行并行加速的特性，使用self-attention机制替代RNN的方法在过去已经层出不穷，为之后在视觉领域引入基于Transformer的方法，本节将进一步介绍Transformer的整体结构框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  整体结构\n",
    "\n",
    "Transformer整体还是一种seq2seq的结构，分为encoder和devoder两个部分。根据Google于2017年提出的论文《Attention is all you need》，在每个encoder和decoder中都包含6个相同的module（decoder中的module与encoder中的有些许差异），下图展示了整体框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=./images/Transformer整体结构.png width=60%/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入数据\n",
    "\n",
    "输入数据的格式为一个matrix，其中每一行或每一列为输入句子中的单个单词经过embedding后的向量加上位置编码。\n",
    "\n",
    "即：$input = word-embedding + positional-embedding$\n",
    "\n",
    "其中单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。\n",
    "\n",
    "进一步，虽然self-attention机制和RNN一样使用到了输入句子的全局信息（即每一个单词的预测都会考虑整个句子的其他单词），但是句子中单词间的位置关系却没有考虑到（每一次考虑到都是当前单词和整个句子的全局关系）。而这些有关单词位置的信息在NLP领域中极其重要，因此Transformer使用位置编码将单词的相对位置以及绝对位置保留下来。\n",
    "\n",
    "参考[知乎文章：详解Transformer中第2.2节](https://zhuanlan.zhihu.com/p/338817680)，论文中的位置编码表示为PE，其维度和输入句子embedding的维度相同（为了后期进行concat），PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：\n",
    "\n",
    "$$PE_{（pos, 2i）} = sin(pos / 10000^{2i/d})$$\n",
    "$$PE_{（pos, 2i+1）} = cos(pos / 10000^{2i/d})$$\n",
    "\n",
    "其中，pos 表示单词在句子中的位置，d 表示 PE的维度 (与词 Embedding 一样)，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)。使用这种公式计算 PE 有以下的好处：\n",
    "\n",
    "- 使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。\n",
    "- 可以让模型容易地计算出相对位置，对于固定长度的间距 k，PE(pos+k) 可以用 PE(pos) 计算得到。因为 Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)。\n",
    "\n",
    "最终模型的输入可以表示如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align = \"center\">\n",
    "    <img src = \"./images/input.png\" width = 60%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Module\n",
    "\n",
    "每一个Encoder Module大体由两部分组成，分别是前面的self-attention结构和后面的Feed Forward层（其中self-attention应指multihead self-attention即多头注意力机制，多头注意力机制可以使得模型区分不同位置的重点）。而Feed Forward层则是由两个简单的全连接层组成，其中第一个全连接层使用RelU作为激活函数，第二层全连接层则不使用任何激活函数，这样使得第二层全连接层相当于一个简单的线性变换，可以加快运算速度。其计算公式如下：$FFD(x) = Max(0, XW_{1}+b_1)W_{2} + b_{2}$，其中X为输入数据。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
