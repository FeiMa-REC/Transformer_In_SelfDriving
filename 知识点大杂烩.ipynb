{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 待读\n",
    "\n",
    "- [Occupancy Network 系列论文汇总](https://zhuanlan.zhihu.com/p/620907153)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础知识扫盲\n",
    "\n",
    "**CNN的两大特性：局部感知和权值共享** [参考博客园](https://www.cnblogs.com/chenshan-crystal/p/7543780.html) \n",
    "\n",
    "局部感知是指CNN在通过滑动窗口提取特征的时候，由于一个卷积核的大小是有限的，通常是3x3或者7x7，那该卷积核就只能学习提取该局部大小的特征。这个特性和图像中真实的特征分布很接近，因为通常在一幅图像中相邻像素之间的相关性总是要远远大于相距甚远的两个像素之间的相关性。因此在一个网络中，前几层卷积层通常学习到的是一些图像的浅层特征，例如物体的边缘特征、点线面等，而在网络更深的层中，由于浅层特征的堆叠那么更高维度，表征能力更强的，更抽象的的特征也可被网络学习和认识，这也解释了为什么在视觉领域前期往往更深的网络效果越好。\n",
    "\n",
    "权值共享是指，例如3x3大小的卷积核学习到的是图像中物体的边缘信息，那么该卷积在滑动窗口的过程中就会不断的提取当前窗口（局部）的边缘特征。即一个卷积核的权值对应于提取某一属性的特征，该卷积核窗口滑动的过程即为权值共享过程，这也从一个方面解释了为什么在视觉领域，通常输入图像的初始维度为3而后面几层卷积层会将维度迅速提升到512或者更高，因为需要更多的卷积核去学习提取不同属性的特征，从而充分利用当前图像存在的特征信息。\n",
    "\n",
    "**CNN的平移不变形** [参考知乎文章](https://zhuanlan.zhihu.com/p/382926269) \n",
    "\n",
    "简单来说就是CNN对图像中的某一物体的识别结果（该物体的特征提取结果）不会随着该物体在图像中发生平移而改变。\n",
    "\n",
    "**归纳偏置（Inductive Bias）** [参考知乎回答](https://www.zhihu.com/question/264264203)\n",
    "\n",
    "\n",
    "该名词在阅读 Vision Transformer 时遇到，知乎回答大体思想就是：“选择某一方法解决问题，而该方法是基于某某假设的。该假设在实际情况下有部分样本不能其假设条件从而造成方法失效，即为归纳偏置。” 例如CNN方法具有的平移不变性在语义分割时就具有缺陷，此时这种不变性即可称之为一种Inductive Bias。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动驾驶发展\n",
    "\n",
    "来自[知乎回答：occupancy network对自动驾驶能带来多大的提升？会让激光雷达变得鸡肋吗？](https://www.zhihu.com/question/614057120/answer/3255853622)\n",
    "\n",
    "自动驾驶方案之争一直在持续，不分高低，谁给用户带来的体验最好，谁就有话语权，大致梳理了下自动驾驶方案的演变路径；\n",
    "- 2015年深度学习的横空出世，带来了图像领域的变革；\n",
    "- 2017-2018年GPU硬件能力的提升，大规模数据集的构建能力，同时神经网络的不断演变（分类-->检测-->分割）等，促使视觉在自动驾驶领域效果显著，特斯拉成为第一波展示其能力的车企，将硬件能力和软件能力发挥到极致；\n",
    "- 2018-2020年，视觉似乎遇到瓶颈，2D图像空间的检测带来的问题明显，无法做到3D空间的准确映射，在测距测速上存在弊端，同时面对一些未知障碍物检测性能也不佳，这些恰好是激光雷达的优势，这几年也是激光雷达迸发的高峰期；\n",
    "- 2020-2022年，堆高算力芯片，堆多个摄像头、堆激光雷达，全方位覆盖自车的360°区域，自动驾驶哪一环缺啥补啥，这一思路没有问题，带来的影响是额外的高成本以及整个系统开始变得极其复杂起来，目前能够搞定这些传感器加算法适配的车企并不多，而且价格也不菲；\n",
    "- 2022-2023年，还是特斯拉，既然视觉2D检测存在瓶颈，2D到3D映射不准，推出BEV+ Transform网络，直接在BEV空间上建模，统一完整的全局场景的表示，物体的大小和朝向都能直接得到表达；但视觉最棘手的长尾问题仍是各类未知的障碍物，Occupancy Network的出现，直接对环境进行建模，通过体素（Voxel）化的方式理解和处理空间信息，感知系统可以对3D物理空间的可通行区域进行高保真度还原，不过度区分语义、更关注是否占用，是否为空，这样就可以去识别任意形状的物体（比如挂车、树木、垃圾等）和任意形式的物体运动，从而从根本上避免传统视觉对非训练集内物体的漏检问题，使模型的泛化能力大幅提升，能更好适应不同场景和环境。相比激光雷达产生的稀疏且不连续的点云，高清摄像头采集的信息内容更丰富，让占用网络更好地将3D几何信息与语义信息融合，帮助汽车机器人更准确还原3D场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 好文\n",
    "\n",
    "1. [关于一个CV领域博士毕业的这件小事](https://mp.weixin.qq.com/s/0iOUDHZ16REiqvuWM-nuPA)\n",
    "\n",
    "文中提到的科研思维很值得学习和借鉴：**我是否足够深入地思考这个领域有哪些真正的痛点问题？已有的问题本身是否就有问题？**\n",
    "\n",
    "如何培养上述科研思维，作者也在文章中分享了四点方法。\n",
    "\n",
    "- **学会批判。** 无论是你眼中的大佬同行，或者你的导师，他们的观点都有可能是错的。你读到的论文里面那些精彩的故事可能都是凑巧编出来的，换一个数据集或者环境可能就完全不work了。看待一切的research work，哪怕是CVPR best paper这样的文章都要带着批判的眼光去看：他们还有哪些问题没有考虑到？他们的设计是否还有瑕疵？他们那精彩的故事是否在某个特定条件下就完全失效了？\n",
    "- **永远不要限制你的想象力，想的越大胆越 有奇效。** 在2021年初，自动驾驶BEV鸟瞰图感知还完全没有一点影子的时候，我文科专业的女友曾经就问我，为什么一定要用LiDAR，直接让AI把相机信息变成俯视图的一个判断不就行了吗？我当时很嗤之以鼻，说这个太难了，相机只有2D，直接用网络推测俯视图结果肯定很烂。结果没过多久特斯拉就公布了它们的方案，和我女友所讲的思路是一致的。\n",
    "- **多读好论文，而且读的要广。** 正如古人所言，思而不学则怠。只是空想永远锻炼不了你的科研思维，一定要多读论文再结合自己的思考才有用，而且很多时候要跨领域去读，譬如现在大火的vision transformer很多都是从NLP借鉴来的。\n",
    "- **多和其他resaercher交流。** 这不仅仅限于自己实验室，其实天南海北，从北极到赤道的人你都可以去交流。有时候你会惊奇的发现， 一封邮件会带来很多新的朋友。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
